{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install_deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in .\\venv\\lib\\site-packages (0.116.1)\n",
      "Requirement already satisfied: scikit-learn in .\\venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in .\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: uvicorn[standard] in .\\venv\\lib\\site-packages (0.35.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in .\\venv\\lib\\site-packages (from fastapi) (0.47.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in .\\venv\\lib\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in .\\venv\\lib\\site-packages (from fastapi) (4.14.1)\n",
      "Requirement already satisfied: click>=7.0 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (0.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (0.4.6)\n",
      "Requirement already satisfied: httptools>=0.6.3 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (1.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (6.0.2)\n",
      "Requirement already satisfied: watchfiles>=0.13 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in .\\venv\\lib\\site-packages (from uvicorn[standard]) (15.0.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in .\\venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in .\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in .\\venv\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in .\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in .\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in .\\venv\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in .\\venv\\lib\\site-packages (from httpx) (4.10.0)\n",
      "Requirement already satisfied: certifi in .\\venv\\lib\\site-packages (from httpx) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\lib\\site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\lib\\site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\lib\\site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in .\\venv\\lib\\site-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in .\\venv\\lib\\site-packages (from anyio->httpx) (4.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install fastapi \"uvicorn[standard]\" scikit-learn numpy\n",
    "\n",
    "! pip install httpx\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SOPPreferences(BaseModel):\n",
    "\n",
    "    field: Optional[str] = Field(None, description=\"Primary technical field, e.g., 'Artificial Intelligence'\")\n",
    "\n",
    "    priority: Optional[str] = Field(None, description=\"High/Medium/Low\")\n",
    "\n",
    "    review_style: Optional[str] = Field(None, description=\"Brief / Detailed / Line-by-line\")\n",
    "\n",
    "\n",
    "\n",
    "class SubmitSOPRequest(BaseModel):\n",
    "\n",
    "    client_id: str\n",
    "\n",
    "    sop_text: str\n",
    "\n",
    "    preferences: Optional[SOPPreferences] = None\n",
    "\n",
    "\n",
    "\n",
    "class MatchItem(BaseModel):\n",
    "\n",
    "    reviewer_id: str\n",
    "\n",
    "    name: str\n",
    "\n",
    "    expertise: List[str]\n",
    "\n",
    "    score: float\n",
    "\n",
    "    breakdown: Dict[str, float]\n",
    "\n",
    "\n",
    "\n",
    "class MatchResponse(BaseModel):\n",
    "\n",
    "    client_id: str\n",
    "\n",
    "    top_k: int\n",
    "\n",
    "    matches: List[MatchItem]\n",
    "\n",
    "\n",
    "\n",
    "class Reviewer(BaseModel):\n",
    "\n",
    "    reviewer_id: str\n",
    "\n",
    "    name: str\n",
    "\n",
    "    expertise_tags: List[str]\n",
    "\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "    max_capacity: int = 5\n",
    "\n",
    "    current_load: int = 0  # for availability factor\n",
    "\n",
    "\n",
    "\n",
    "# sop_id to data\n",
    "\n",
    "SOP: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "\n",
    "\n",
    "# client_id to [sop_ids]\n",
    "\n",
    "CLIENT_TO_SOP: Dict[str, List[str]] = {}\n",
    "\n",
    "\n",
    "\n",
    "# reviewer_id to Reviewer\n",
    "\n",
    "REVIEWERS: Dict[str, Reviewer] = {}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tags(tags: List[str]) -> List[str]:\n",
    "    return [re.sub(r\"\\s+\", \" \", t.strip().lower()) for t in tags]\n",
    "\n",
    "\n",
    "\n",
    "def reviewer_corpus_text(r: Reviewer) -> str:\n",
    "    base = \" \".join(normalize_tags(r.expertise_tags))\n",
    "    return (base + \" \" + (r.notes or \"\")).strip()\n",
    "\n",
    "\n",
    "\n",
    "def sop_doc_text(sop_text: str, prefs: Optional[SOPPreferences]) -> str:\n",
    "    field_part = (prefs.field if prefs and prefs.field else \"\")\n",
    "    return f\"{sop_text}\\n{field_part}\"\n",
    "\n",
    "\n",
    "\n",
    "def tag_overlap_score(sop_text: str, reviewer_tags: List[str]) -> float:\n",
    "\n",
    "    text = sop_text.lower()\n",
    "\n",
    "    tags = normalize_tags(reviewer_tags)\n",
    "\n",
    "    if not tags:\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    hits = sum(1 for t in tags if t in text)\n",
    "\n",
    "    return hits / len(tags)\n",
    "\n",
    "\n",
    "\n",
    "def availability_factor(current_load: int, max_capacity: int) -> float:\n",
    "\n",
    "    if max_capacity <= 0:\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    val = 1.0 - (current_load / max_capacity)\n",
    "\n",
    "    return max(0.0, min(1.0, val))\n",
    "\n",
    "\n",
    "\n",
    "def compute_scores_for_sop(sop_text: str, prefs: Optional[SOPPreferences]) -> List[Dict[str, Any]]:\n",
    "\n",
    "    if not REVIEWERS:\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    sop_doc = sop_doc_text(sop_text, prefs)\n",
    "\n",
    "    reviewer_docs = [reviewer_corpus_text(r) for r in REVIEWERS.values()]\n",
    "\n",
    "    corpus = [sop_doc] + reviewer_docs\n",
    "\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), max_features=5000)\n",
    "\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "\n",
    "    p_vec = X[0]           # 1 x V\n",
    "\n",
    "    r_vecs = X[1:]         # R x V\n",
    "\n",
    "\n",
    "\n",
    "    # Cosine similarities\n",
    "\n",
    "    sims = cosine_similarity(p_vec, r_vecs).flatten()\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare scores for each reviewer\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, (rid, r) in enumerate(REVIEWERS.items()):\n",
    "\n",
    "        content_similarity = float(sims[idx])  # already 0..1 for tf-idf cosine\n",
    "\n",
    "        exp_match = tag_overlap_score(sop_doc, r.expertise_tags)  # 0..1\n",
    "\n",
    "        avail = availability_factor(r.current_load, r.max_capacity)    # 0..1\n",
    "\n",
    "\n",
    "\n",
    "        # Weighted score\n",
    "\n",
    "        score = (0.5 * content_similarity) + (0.3 * exp_match) + (0.2 * avail)\n",
    "\n",
    "\n",
    "\n",
    "        results.append({\n",
    "\n",
    "            \"reviewer_id\": rid,\n",
    "\n",
    "            \"name\": r.name,\n",
    "\n",
    "            \"expertise\": r.expertise_tags,\n",
    "\n",
    "            \"score\": float(score),\n",
    "\n",
    "            \"breakdown\": {\n",
    "\n",
    "                \"content_similarity\": float(content_similarity),\n",
    "\n",
    "                \"expertise_match\": float(exp_match),\n",
    "\n",
    "                \"availability\": float(avail)\n",
    "\n",
    "            }\n",
    "\n",
    "        })\n",
    "\n",
    "    # Sort by score desc\n",
    "\n",
    "    results.sort(key=lambda d: d[\"score\"], reverse=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Profile matching\", version=\"0.1.0\")\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/submit-sop\")\n",
    "\n",
    "def submit_sop(req: SubmitSOPRequest):\n",
    "\n",
    "    # Basic validation\n",
    "\n",
    "    if not req.client_id or not req.sop_text.strip():\n",
    "\n",
    "        raise HTTPException(status_code=400, detail=\"client_id and sop_text are required.\")\n",
    "\n",
    "\n",
    "\n",
    "    sop_id = str(uuid4())\n",
    "\n",
    "    SOP[sop_id] = {\n",
    "\n",
    "        \"sop_id\": sop_id,\n",
    "\n",
    "        \"client_id\": req.client_id,\n",
    "\n",
    "        \"sop_text\": req.sop_text,\n",
    "\n",
    "        \"preferences\": req.preferences.model_dump() if req.preferences else None,\n",
    "\n",
    "        \"created_at\": datetime.datetime.now(datetime.UTC)\n",
    "\n",
    "    }\n",
    "\n",
    "    CLIENT_TO_SOP.setdefault(req.client_id, []).append(sop_id)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"sop submitted successfully\", \"sop_id\": sop_id}\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/match/{client_id}\", response_model=MatchResponse)\n",
    "\n",
    "def match_reviewers(client_id: str, top_k: int = 5):\n",
    "\n",
    "    sop_ids = CLIENT_TO_SOP.get(client_id)\n",
    "\n",
    "    if not sop_ids:\n",
    "\n",
    "        raise HTTPException(status_code=404, detail=\"No sop found for this client_id.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Use the latest sop for this client\n",
    "\n",
    "    last_pid = sop_ids[-1]\n",
    "\n",
    "    pdata = SOP[last_pid]\n",
    "\n",
    "\n",
    "\n",
    "    scores = compute_scores_for_sop(\n",
    "\n",
    "        sop_text=pdata[\"sop_text\"],\n",
    "\n",
    "        prefs=SOPPreferences(**pdata[\"preferences\"]) if pdata[\"preferences\"] else None\n",
    "\n",
    "    )\n",
    "\n",
    "    if not scores:\n",
    "\n",
    "        return MatchResponse(client_id=client_id, top_k=0, matches=[])\n",
    "\n",
    "\n",
    "\n",
    "    matches = [\n",
    "\n",
    "        MatchItem(\n",
    "\n",
    "            reviewer_id=s[\"reviewer_id\"],\n",
    "\n",
    "            name=s[\"name\"],\n",
    "\n",
    "            expertise=s[\"expertise\"],\n",
    "\n",
    "            score=round(s[\"score\"], 4),\n",
    "\n",
    "            breakdown={k: round(v, 4) for k, v in s[\"breakdown\"].items()}\n",
    "\n",
    "        )\n",
    "\n",
    "        for s in scores[:max(1, top_k)]\n",
    "    ]\n",
    "\n",
    "    return MatchResponse(client_id=client_id, top_k=len(matches), matches=matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seed_reviewers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rev001', 'rev002', 'rev003', 'rev004']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_reviewers():\n",
    "\n",
    "    REVIEWERS.clear()\n",
    "\n",
    "    r1 = Reviewer(\n",
    "\n",
    "        reviewer_id=\"rev001\",\n",
    "\n",
    "        name=\"Dr. Amina Rahman\",\n",
    "\n",
    "        expertise_tags=[\"SOP Evaluation\", \"Academic Writing\", \"Artificial Intelligence\", \"Machine Learning\", \"Technical Research\"],\n",
    "\n",
    "        notes=\"Expert in evaluating AI/ML SOPs; specializes in technical narrative coherence and research impact assessment.\",\n",
    "\n",
    "        max_capacity=6,\n",
    "\n",
    "        current_load=2,\n",
    "\n",
    "    )\n",
    "\n",
    "    r2 = Reviewer(\n",
    "\n",
    "        reviewer_id=\"rev002\",\n",
    "\n",
    "        name=\"John Carter\",\n",
    "\n",
    "        expertise_tags=[\"SOP Evaluation\", \"Biomedical Research\", \"Public Health\", \"Academic Writing\", \"Evidence Analysis\"],\n",
    "\n",
    "        notes=\"Experienced with research narrative evaluation and academic impact assessment for STEM applications.\",\n",
    "\n",
    "        max_capacity=5,\n",
    "\n",
    "        current_load=4,\n",
    "\n",
    "    )\n",
    "\n",
    "    r3 = Reviewer(\n",
    "\n",
    "        reviewer_id=\"rev003\",\n",
    "\n",
    "        name=\"Sophia Lee\",\n",
    "\n",
    "        expertise_tags=[\"SOP Evaluation\", \"Civil Engineering\", \"Infrastructure\", \"Transportation\", \"Environmental Research\"],\n",
    "\n",
    "        notes=\"Strong with technical project narratives and infrastructure research impact evaluation.\",\n",
    "\n",
    "        max_capacity=4,\n",
    "\n",
    "        current_load=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    r4 = Reviewer(\n",
    "\n",
    "        reviewer_id=\"rev004\",\n",
    "\n",
    "        name=\"Miguel Alvarez\",\n",
    "\n",
    "        expertise_tags=[\"SOP Evaluation\", \"Economics\", \"Entrepreneurship\", \"Business Research\", \"Commercial Impact\"],\n",
    "\n",
    "        notes=\"Focus on market impact narratives, business case analysis, and commercial potential assessment.\",\n",
    "\n",
    "        max_capacity=3,\n",
    "\n",
    "        current_load=0,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    for r in (r1, r2, r3, r4):\n",
    "\n",
    "        REVIEWERS[r.reviewer_id] = r\n",
    "\n",
    "\n",
    "\n",
    "seed_reviewers()\n",
    "\n",
    "list(REVIEWERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "test_client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit status: 200\n",
      "{'status': 'success', 'message': 'sop submitted successfully', 'sop_id': '47dbab91-2285-499d-bbd3-2adbb4114ea5'}\n",
      "\n",
      "Match status: 200\n",
      "{'client_id': 'client_123', 'top_k': 3, 'matches': [{'reviewer_id': 'rev001', 'name': 'Dr. Amina Rahman', 'expertise': ['SOP Evaluation', 'Academic Writing', 'Artificial Intelligence', 'Machine Learning', 'Technical Research'], 'score': 0.2338, 'breakdown': {'content_similarity': 0.0809, 'expertise_match': 0.2, 'availability': 0.6667}}, {'reviewer_id': 'rev003', 'name': 'Sophia Lee', 'expertise': ['SOP Evaluation', 'Civil Engineering', 'Infrastructure', 'Transportation', 'Environmental Research'], 'score': 0.2288, 'breakdown': {'content_similarity': 0.0377, 'expertise_match': 0.2, 'availability': 0.75}}, {'reviewer_id': 'rev004', 'name': 'Miguel Alvarez', 'expertise': ['SOP Evaluation', 'Economics', 'Entrepreneurship', 'Business Research', 'Commercial Impact'], 'score': 0.2022, 'breakdown': {'content_similarity': 0.0043, 'expertise_match': 0.0, 'availability': 1.0}}]}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "\n",
    "\n",
    "payload = {\n",
    "\n",
    "    \"client_id\": \"client_123\",\n",
    "\n",
    "    \"sop_text\": (\n",
    "\n",
    "        \"My research in artificial intelligence focuses on robust perception for autonomous systems \"\n",
    "\n",
    "        \"with applications in national security and critical infrastructure resilience. \"\n",
    "\n",
    "        \"I collaborate with US defense labs and have publications at NeurIPS and ICRA. \"\n",
    "\n",
    "        \"My work enables safer autonomous navigation and situational awareness.\"\n",
    "\n",
    "    ),\n",
    "\n",
    "    \"preferences\": {\n",
    "\n",
    "        \"field\": \"Artificial Intelligence, Autonomous Systems, National Security\",\n",
    "\n",
    "        \"priority\": \"High\",\n",
    "\n",
    "        \"review_style\": \"Detailed\"\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "resp_submit = client.post(\"/submit-sop\", json=payload)\n",
    "\n",
    "print(\"Submit status:\", resp_submit.status_code)\n",
    "\n",
    "print(resp_submit.json())\n",
    "\n",
    "\n",
    "\n",
    "resp_match = client.get(\"/match/client_123\", params={\"top_k\": 3})\n",
    "\n",
    "print(\"\\nMatch status:\", resp_match.status_code)\n",
    "\n",
    "print(resp_match.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
